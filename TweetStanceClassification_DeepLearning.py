# -*- coding: utf-8 -*-
"""Assignment2_DL_s3880522.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10sj0u_ZCEUsWvs_liDHDFGsA_pd2bVeR

# Deep Learning (COSC 2779) - Assignment 2


---

###  Topic: Sequence Processing with Deep learning [Tweet Stance Classification]


---


##[Submitted by Jyoti , s3880522 ] RMIT University , 16 Oct 2023

#Introduction and Problem Definition

>Stance classification is a specialized subfield of opinion mining that focuses on automatically determining whether a piece of text expresses support or opposition toward a specific target or proposition. It finds extensive applications in information retrieval, text summarization, and textual entailment. Distinction must be made between stance classification and sentiment analysis. While sentiment analysis assesses the overall positivity, negativity, or neutrality of text, stance classification contextualizes the text with respect to a target topic. This means that a piece of text's stance can vary independently of the emotional tone of the language used.
Understanding stance is vital for various real-world applications, including tailoring information retrieval results, summarizing text with diverse opinions, assessing logical relationships between texts, and analyzing public sentiment.

#Objective

> The objective of project is to create a deep learning model for classification of tweets centered around five politically-charged topics: "Atheism," "the Feminist Movement," "Climate Change is a Real Concern," "Legalization of Abortion," and "Hillary Clinton." The model's task is to categorize each tweet into one of three classes: "FAVOUR," indicating support for the target; "AGAINST," indicating opposition to the target; or "NEITHER," signifying a neutral stance with no clear support or opposition.
The original dataset used in this project for stance classification of tweets is sourced from the "Semeval-2016 Task 6: Detecting Stance in Tweets "developed by Saif M. Mohammad and his team and was introduced as part of the International Workshop on Semantic Evaluation (SemEval-16) in June 2016 in a series of workshops that focus on various natural language processing and computational linguistics tasks.


##### Copyright 2020 The TensorFlow Hub Authors.

#About BERT
BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.

BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.
"""

# A dependency of the preprocessing for BERT inputs
!pip install -U "tensorflow-text==2.13.*"

!pip install "tf-models-official==2.13.*"

"""This code prepares the environment for working with TensorFlow, data processing, and data visualization in a deep learning project."""

import tensorflow as tf
AUTOTUNE = tf.data.experimental.AUTOTUNE
import numpy as np
import pandas as pd

import tensorflow_datasets as tfds
import pathlib
import shutil
import tempfile

from  IPython import display
from matplotlib import pyplot as plt

import os
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')

from google.colab import drive
drive.mount('/content/drive')

"""The provided code snippet is specific to the Google Colab environment and is used for mounting Google Drive into the Colab notebook. Here's a summary of what the code does:

It imports the drive module from the google.colab library.
It uses the drive.mount() function to mount the Google Drive to the Colab environment.

#Loading Data Set
The original dataset used in this project for stance classification of tweets is sourced from the "Semeval-2016 Task 6: Detecting Stance in Tweets "developed by Saif M. Mohammad and his team and was introduced as part of the International Workshop on Semantic Evaluation (SemEval-16) in June 2016 in a series of workshops that focus on various natural language processing and computational linguistics tasks.

#EDA:  Exploratory Data Analysis
"""

import pandas as pd
import re
import re
import string

# Preprocessing function
def preprocess_tweet(tweet):
    tweet = tweet.lower()
    tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet)
    tweet = re.sub(f"[{re.escape(string.punctuation)}]", '', tweet)
    tweet = re.sub(r'\d', '', tweet)
    return tweet

# Load train and test data
data_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/train.csv', sep=',', lineterminator='\r', encoding='unicode_escape')
data_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/test.csv', sep=',', lineterminator='\r', encoding='unicode_escape')

# Apply preprocessing to the 'Tweet' column in both train and test data
data_train['Tweet'] = data_train['Tweet'].apply(preprocess_tweet)
data_test['Tweet'] = data_test['Tweet'].apply(preprocess_tweet)

# Save the cleaned data as new CSV files
data_train.to_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/TrainFileCleaned.csv', index=False)
data_test.to_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/TestFileCleaned.csv', index=False)

#Display rows and columns in the datasets
print("Training Set")
print('Number of Rows= {}'.format(data_train.shape[0]))
print('Number of Columns= {}'.format(data_train.shape[1]))
print("Test Set")
print('Number of Rows= {}'.format(data_test.shape[0]))
print('Number of Columns= {}'.format(data_test.shape[1]))

# Display the first few rows of the DataFrame to verify
data_train.head()

data_test.head()

# Check data types of columns
data_train.dtypes

# Summary statistics
data_train.describe()

# Summary statistics
data_test.describe()

data_train["Stance"].value_counts()

data_test["Stance"].value_counts()

#Filter out non target and retain only specified targets in test data
#TARGETS : {"Hillary Clinton", "Legalization of Abortion", "Atheism", "Climate Change is a Real Concern", "Feminist Movement"  }
data_test= pd.read_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/TestFileCleaned.csv',sep=',',encoding = 'unicode_escape')
filt=data_train['Target'].unique()
data_test = data_test[data_test.Target.isin(filt)]
data_test.head()

#Print unique stances
data_train.Stance.unique()

#Print unique target
data_train.Target.unique()

data_train.head()

# Check for null values in the training dataset
null_values_train = data_train.isnull().sum()

# Check for duplicated rows in the training dataset
duplicates_train = data_train.duplicated().sum()

# Check for null values in the test dataset
null_values_test = data_test.isnull().sum()

# Check for duplicated rows in the test dataset
duplicates_test = data_test.duplicated().sum()

print("Null values in the training dataset:")
print(null_values_train)
print("\nDuplicated rows in the training dataset:", duplicates_train)

print("\nNull values in the test dataset:")
print(null_values_test)
print("\nDuplicated rows in the test dataset:", duplicates_test)

"""##Data Visualization"""

#Visualize some aspects of the data
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Plot the distribution of stance categories in the train data
plt.figure(figsize=(8, 5))
sns.countplot(data=data_train, x='Stance', palette='viridis')
plt.title('Distribution of Stance Categories in Train Data')
plt.xlabel('Stance Categories')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Plot the distribution of stance categories in the test data
plt.figure(figsize=(8, 5))
sns.countplot(data=data_test, x='Stance', palette='viridis')
plt.title('Distribution of Stance Categories in Test Data')
plt.xlabel('Stance Categories')
plt.ylabel ('Count')
plt.xticks(rotation=45)
plt.show()

# Visualize the tweet lengths in both train and test data
data_train['Tweet Length'] = data_train['Tweet'].apply(lambda x: len(x.split()))
data_test['Tweet Length'] = data_test['Tweet'].apply(lambda x: len(x.split()))

plt.figure(figsize=(12, 6))
sns.kdeplot(data=data_train['Tweet Length'], label='Train Data', shade=True)
sns.kdeplot(data=data_test['Tweet Length'], label='Test Data', shade=True)
plt.title('Distribution of Tweet Length in Train and Test Data')
plt.xlabel('Tweet Length')
plt.ylabel('Density')
plt.legend()
plt.show()

#Plot number of tweets in each target
import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
sns.set(rc={'figure.figsize': (15, 5)})
sns.set_theme(style="darkgrid")

# Define a custom color palette
custom_palette = sns.color_palette("Set2")  # You can choose any color palette

# Create the countplot with the custom color palette
ax = sns.countplot(x="Target", hue="Target", data=data_train, palette=custom_palette)

# Customize the plot
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title("Number of Tweets in Each Target")

# Show the plot
plt.show()

#Plot number of tweets in each stance
import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
sns.set(rc={'figure.figsize': (9, 5)})

# Define a custom color palette
custom_palette = sns.color_palette("husl")  # You can choose any color palette

# Create the countplot with the custom color palette
ax = sns.countplot(x="Stance", hue="Stance", data=data_train, palette=custom_palette)

# Customize the plot
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title("Number of Tweets in Each Stance")

# Show the plot
plt.show()

#Plot number of tweets with opinions
import seaborn as sns
import matplotlib.pyplot as plt

# Set the figure size
sns.set(rc={'figure.figsize': (15, 5)})

# Define a custom color palette
custom_palette = sns.color_palette("Set3")  # You can choose any color palette

# Create the countplot with the custom color palette
ax = sns.countplot(x="Target", hue="Opinion Towards", data=data_train, palette=custom_palette)

# Customize the plot
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title("Number of Tweets with Opinions by Target")

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.show()

import nltk
nltk.download('wordnet')

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
import re
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Download NLTK stop words if you haven't already
nltk.download('stopwords')
nltk.download('punkt')

custom_stopwords = ["the", "and", "of", "to", "a", "in", "is", "for", "on", "with", "that", "this", "it", "as" , "semst" ]

# Define a function to preprocess text
def preprocess_text(text):
    # Lowercasing
    text = text.lower()

    # Removing special characters and numbers
    text = re.sub(r'[^A-Za-z ]+', '', text)

    # Remove hashtags
    text = re.sub(r'#\w+', '', text)

    # Replace % and $ symbols with words
    text = text.replace('%', 'percent')
    text = text.replace('$', 'dollar')

    # Replace digits with words (e.g., "123" with "one two three")
    text = ' '.join([word if not word.isdigit() else ' '.join(list(word)) for word in text.split()])

    # Remove non-ASCII characters
    text = ''.join(char for char in text if ord(char) < 128)

    # Tokenization
    words = word_tokenize(text)

    # Remove custom stop words
    filtered_words = [word for word in words if word not in custom_stopwords]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]

    return ' '.join(lemmatized_words)

# Function to display a word cloud for a given target
def make_word_cloud(text, title):
    wcloud = WordCloud(background_color="white", max_words=50, stopwords=STOPWORDS)
    wcloud.generate(text)

    plt.figure(figsize=(8, 8))
    plt.imshow(wcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

# Visualize the most occurring words in each target
target_vals = data_train['Target'].unique()

# Iterate through each target class
for val in target_vals:
    tar_val = data_train.loc[data_train['Target'] == val]
    all_target_val = " ".join(tar_val.Tweet.values)

    # Preprocess text for this target class
    all_target_val = preprocess_text(all_target_val)

    # Create a word cloud and display it with the actual target value as the title
    title = f"Word Cloud for Target: {val}"  # Include the actual target value
    make_word_cloud(all_target_val, title)
# Provide an explanation of the process
print("Word clouds generated for each target class. Hashtags removed, symbols replaced with words, and digits replaced with words.")
print("Non-ASCII characters removed to ensure the model doesn't encounter unrecognized characters.")
print("This preprocessing helps visualize the most frequent keywords in the tweets.")
print("The hashtag '#SemST' has been discarded, as suggested by Isabelle Augenstein et al. [6].")

data_train['Target']

"""## Data Imbalance Overview"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate data imbalance information
imbalance_info_train = data_train['Stance'].value_counts()
imbalance_info_test = data_test['Stance'].value_counts()

# Print data imbalance information
print("Data Imbalance Info in Training Data:")
print(imbalance_info_train)
print("\nData Imbalance Info in Testing Data:")
print(imbalance_info_test)

# Create a bar plot to visualize data imbalance in training data
sns.set(rc={'figure.figsize': (8, 6)})
sns.set_theme(style="darkgrid")

# Plot the data imbalance
ax = sns.countplot(x="Stance", data=data_train, palette="Set2")

# Customize the plot
plt.title("Data Imbalance in Training Data")
plt.xlabel("Stance")
plt.ylabel("Number of Tweets")

# Show the plot
plt.show()

"""> Result Analysis : The provided data imbalance information for both the training and testing datasets indicates the distribution of tweets across different stance categories. In this context:

- For the Training Data:

"AGAINST" stance has 1395 instances.
"NONE" stance has 766 instances.
"FAVOR" stance has 753 instances.

- For the Testing Data:

"AGAINST" stance has 715 instances.
"FAVOR" stance has 304 instances.
"NONE" stance has 230 instances.
Data Imbalance Overview:

> The data imbalance in this case is evident as there is a significant difference in the number of instances between the "AGAINST" stance and the other stances ("NONE" and "FAVOR"). This can impact the performance of machine learning models, as they may become biased towards the majority class (in this case, "AGAINST").

Options to Address Data Imbalance:

To address data imbalance in the context of stance classification, several options can be considered:

- Resampling: One common approach is to either oversample the minority classes (e.g., "FAVOR" and "NONE") or undersample the majority class ("AGAINST") to balance the dataset. This can be done through techniques like Random Oversampling, Random Undersampling, or Synthetic Minority Over-sampling Technique (SMOTE).

- Different Evaluation Metrics: Use evaluation metrics that are robust to class imbalance, such as F1-score, precision, recall, and the area under the ROC curve (AUC). These metrics provide a more accurate assessment of model performance in imbalanced datasets.

- Cost-Sensitive Learning: Modify the learning algorithm to assign different misclassification costs to different classes. This can be done by adjusting class weights in the model.

- Collect More Data: If feasible, collecting more data for the minority classes can help balance the dataset.

- Data Augmentation: Generate synthetic data points for minority classes to balance the dataset. This can be achieved by adding variations to existing data.

#Model Evaluation

## Baseline - Loading models from TensorFlow Hub
"In the context of fine-tuning BERT models from TensorFlow Hub, I have a diverse range of options to choose from. These options include various BERT models with distinct characteristics and use cases. Below is a summarized version for my report:

- BERT-Base, Uncased: This is the original BERT model with trained weights, providing a solid baseline for various natural language processing tasks.

- Small BERTs: These models retain the general BERT architecture but have fewer and smaller Transformer blocks. They offer a way to balance factors like model speed, size, and performance.

- ALBERT: ALBERT stands for "A Lite BERT" and comes in four different sizes. It reduces the model's size while maintaining its effectiveness by sharing parameters between layers.

- BERT Experts: This set of models utilizes the BERT-base architecture but is fine-tuned on different pre-training domains. This enables alignment with specific target tasks and domains.

- Electra: While sharing the same architecture as BERT, Electra is pre-trained differently as a discriminator in a setup similar to a Generative Adversarial Network (GAN).

- BERT with Talking-Heads Attention and Gated GELU: These models introduce two core improvements to the Transformer architecture, with variations in size (base, large).

The recommendation is to start with Small BERT models for faster fine-tuning. If I desire a smaller model with higher accuracy, ALBERT is a suitable option. For enhanced accuracy, I can choose from classic BERT models or their recent refinements such as Electra, Talking Heads, or BERT Experts.It's important to note that larger versions of these models can offer even better accuracy but may require more computational resources and are typically fine-tuned on TPUs for efficiency.

I can easily switch between these models by changing the TensorFlow Hub URL, as the differences between them are encapsulated in the SavedModels from TF Hub."
"""

#@title Choose a BERT model

bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param ["bert_en_uncased_L-12_H-768_A-12", "bert_en_cased_L-12_H-768_A-12", "bert_multi_cased_L-12_H-768_A-12", "small_bert/bert_en_uncased_L-2_H-128_A-2", "small_bert/bert_en_uncased_L-2_H-256_A-4", "small_bert/bert_en_uncased_L-2_H-512_A-8", "small_bert/bert_en_uncased_L-2_H-768_A-12", "small_bert/bert_en_uncased_L-4_H-128_A-2", "small_bert/bert_en_uncased_L-4_H-256_A-4", "small_bert/bert_en_uncased_L-4_H-512_A-8", "small_bert/bert_en_uncased_L-4_H-768_A-12", "small_bert/bert_en_uncased_L-6_H-128_A-2", "small_bert/bert_en_uncased_L-6_H-256_A-4", "small_bert/bert_en_uncased_L-6_H-512_A-8", "small_bert/bert_en_uncased_L-6_H-768_A-12", "small_bert/bert_en_uncased_L-8_H-128_A-2", "small_bert/bert_en_uncased_L-8_H-256_A-4", "small_bert/bert_en_uncased_L-8_H-512_A-8", "small_bert/bert_en_uncased_L-8_H-768_A-12", "small_bert/bert_en_uncased_L-10_H-128_A-2", "small_bert/bert_en_uncased_L-10_H-256_A-4", "small_bert/bert_en_uncased_L-10_H-512_A-8", "small_bert/bert_en_uncased_L-10_H-768_A-12", "small_bert/bert_en_uncased_L-12_H-128_A-2", "small_bert/bert_en_uncased_L-12_H-256_A-4", "small_bert/bert_en_uncased_L-12_H-512_A-8", "small_bert/bert_en_uncased_L-12_H-768_A-12", "albert_en_base", "electra_small", "electra_base", "experts_pubmed", "experts_wiki_books", "talking-heads_base"]

map_name_to_handle = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_base/2',
    'electra_small':
        'https://tfhub.dev/google/electra_small/2',
    'electra_base':
        'https://tfhub.dev/google/electra_base/2',
    'experts_pubmed':
        'https://tfhub.dev/google/experts/bert/pubmed/2',
    'experts_wiki_books':
        'https://tfhub.dev/google/experts/bert/wiki_books/2',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',
}

map_model_to_preprocess = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',
    'electra_small':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'electra_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'experts_pubmed':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'experts_wiki_books':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
}

tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')

"""###Model Choosen : Small Bert
I am choosing the "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1" model to start with for tweet stance classification for several reasons:

- Size and Speed: This model is part of the "Small BERTs" family, which means it has a smaller size and fewer Transformer blocks. As a result, it is faster to fine-tune and use in my classification task. This is particularly advantageous for processing large volumes of text data, such as tweets.

- Balanced Trade-offs: Small BERTs offer a balance between speed, model size, and quality. They are well-suited for tasks where computational resources may be limited, but I still want a reasonably effective model.

- Fine-Tuning Flexibility: These smaller models are easier to fine-tune and experiment with due to their reduced complexity. It allows me to quickly iterate and test different approaches for tweet stance classification.

- Applicability to Tweets: Given the inherent brevity of tweets, a smaller BERT model may be sufficient for capturing the context and semantics of tweet text, making it a practical choice for my specific task.

- Starting with a model like this allows me to establish a solid baseline for tweet stance classification. If I find that the model meets my requirements, I can save on computational resources. However, if I need higher accuracy, I can consider more complex models like ALBERT or classic BERT sizes in subsequent iterations. The flexibility of switching between models on TensorFlow Hub enables me to adapt to my specific needs and constraints effectively.

## BERT Model preprocessing and Testing


I will load the preprocessing model into a hub.KerasLayer to compose my fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model.

The preprocessing model is essential for several reasons, especially when working with BERT models like the "small_bert/bert_en_uncased_L-4_H-512_A-8/1" that I've selected for tweet stance classification:

- Tokenization: The preprocessing model takes care of transforming text inputs into numeric token IDs. For BERT models, text needs to be tokenized, which involves splitting text into subword tokens. This is a crucial step for processing natural language text.

- Text to Tensor Conversion: BERT models require text inputs to be arranged into tensors with the appropriate structure. The preprocessing model efficiently converts tokenized text into tensors suitable for BERT's input requirements.

- Standardized Preprocessing: By using the provided preprocessing model, I ensure that the text data is processed consistently and correctly according to the BERT model's expectations. This consistency is crucial for fine-tuning and inference.

- Efficiency: The preprocessing model leverages TensorFlow operations from the TF.text library, making the text transformation process efficient and optimized for integration with BERT models.

In summary, the preprocessing model simplifies and automates the critical task of preparing text data for input to the BERT model. It streamlines the process and ensures that text is correctly tokenized and converted into tensors, enabling efficient fine-tuning and inference. Using this approach also aligns with best practices for working with BERT models in TensorFlow.
"""

bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)

text_test = ['this is such an amazing movie!']
text_preprocessed = bert_preprocess_model(text_test)

print(f'Keys       : {list(text_preprocessed.keys())}')
print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')

"""As I can see, now I have the 3 outputs from the preprocessing that a BERT model would use (input_words_id, input_mask and input_type_ids).

Some other important points:

The input is truncated to 128 tokens. The number of tokens can be customized, and there are more details on the Solve GLUE tasks using BERT on a TPU colab.
The input_type_ids only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.
Since this text preprocessor is a TensorFlow model, It can be included in your model directly.

## BaseLine Model : Using the BERT model

Before putting BERT into  own model, let's take a look at its outputs. You will load it from TF Hub and see the returned values.
"""

bert_model = hub.KerasLayer(tfhub_handle_encoder)

bert_results = bert_model(text_preprocessed)

print(f'Loaded BERT: {tfhub_handle_encoder}')
print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')

"""The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:

- `pooled_output` represents each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire tweet.

- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token ..
- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs["encoder_outputs"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.

For the fine-tuning I am  going to use the `pooled_output` array.

## Define Baseline Model
I will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.


Let's check that the model runs with the output of the preprocessing model.

The output is meaningless, of course, because the model has not been trained yet.

Let's take a look at the model's structure.

The  model consists of input layers for the tweet and target, BERT preprocessing, two BERT encoders for text and target inputs, a concatenation layer to capture the relationship, and additional layers for regularization and classification. This architecture allows the model to effectively learn and classify tweet stances while accommodating both the tweet content and the specific target of interest.
"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Define your BERT model function
def build_custom_bert_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweet')
    target_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='target')

    # BERT preprocessing layer
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

    # Encoder for text input
    text_encoder_inputs = preprocessing_layer(text_input)
    text_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_text')
    text_outputs = text_encoder(text_encoder_inputs)

    # Encoder for target input
    target_encoder_inputs = preprocessing_layer(target_input)
    target_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_target')
    target_outputs = target_encoder(target_encoder_inputs)

    # Concatenate the outputs
    merged = tf.keras.layers.concatenate([text_outputs['pooled_output'], target_outputs['pooled_output']])

    # Additional layers
    net = tf.keras.layers.Dropout(0.1)(merged)
    net = tf.keras.layers.Dense(3, activation='softmax', name='stance')(net)

    return tf.keras.Model(inputs=[text_input, target_input], outputs=net)

# Create your custom BERT model
custom_bert_model = build_custom_bert_model()

# Display the model summary
custom_bert_model.summary()

"""## Baseline Model summary

This model is well-suited for tweet stance classification for the following reasons:

- Dual Input: The model takes two inputs - "tweet" and "target." In tweet stance classification, it's crucial to consider both the content of the tweet and the specific target or topic being discussed. By providing dual inputs, the model can effectively capture the relationship between the tweet and its stance toward the target.

- BERT Preprocessing: The code utilizes the BERT preprocessing layer from TensorFlow Hub to transform the text inputs into numeric token IDs and arrange them into tensors. This is essential for BERT models, as they require standardized text preprocessing.

- Fine-Tuning Capability: The BERT encoder layers for both text and target inputs are fine-tuned. Fine-tuning allows the model to adapt to the specific tweet stance classification task, capturing relevant information from the provided data.

- Concatenation of Outputs: The model concatenates the BERT outputs from both the text and target encoders. This combination of information helps the model understand the relationship between the tweet and the target, which is vital for determining the stance.

- Additional Layers: After the concatenation, the model includes additional layers, such as dropout and a dense layer with softmax activation. These layers further refine and classify the concatenated output into one of three possible stances - FAVOR, AGAINST, or NEITHER.


In summary, this custom BERT model is well-suited for tweet stance classification as it accounts for both the tweet content and the specific target. It leverages the power of BERT's pre-trained encoders while allowing fine-tuning for task-specific adaptation. The model can effectively learn the nuanced relationships between tweets and their stances toward targets, making it a suitable choice for this classification task.
"""

tf.keras.utils.plot_model(custom_bert_model)

"""This output layer consists of a Dense layer with three output units, representing the three possible stances: FAVOR, AGAINST, and NEITHER. The activation function used is softmax, which is commonly used for multi-class classification tasks. It ensures that the output values are normalized and represent the probability distribution over the three classes. The name 'stance' is assigned to this output layer."""

# Compile the model
custom_bert_model.compile(optimizer='adam',
                          loss='categorical_crossentropy',
                          metrics=['accuracy'])

# Split your data into training and validation
train_data, val_data = train_test_split(data_train, test_size=0.2, random_state=42)

# Encode the 'Stance' labels using one-hot encoding
label_encoder = LabelEncoder()
train_data['Stance'] = label_encoder.fit_transform(train_data['Stance'])
val_data['Stance'] = label_encoder.transform(val_data['Stance'])

# Limit the number of training samples (you can adjust this number)
# This can be useful for debugging or quicker training
# Only include this line if you want to limit the training data
train_data = train_data.sample(n=1000, random_state=42)

# Define your input and target data
X_train_text = train_data['Tweet'].values
X_train_target = train_data['Target'].values
y_train = to_categorical(train_data['Stance'].values, num_classes=3)

X_val_text = val_data['Tweet'].values
X_val_target = val_data['Target'].values
y_val = to_categorical(val_data['Stance'].values, num_classes=3)

# Train your model with a reduced batch size
history = custom_bert_model.fit(
    {'tweet': X_train_text, 'target': X_train_target},
    {'stance': y_train},
    validation_data=({'tweet': X_val_text, 'target': X_val_target}, {'stance': y_val}),
    epochs=10,  # You can adjust the number of epochs
    batch_size=32  # Adjust the batch size to your available resources
)

# Extract training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot training and validation loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label="Training Loss")
plt.plot(validation_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label="Training Accuracy")
plt.plot(validation_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Validation Accuracy")

plt.tight_layout()
plt.show()

"""##Evaluating the model"""

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
# Define target names
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']
# Make predictions on the validation set
y_pred = custom_bert_model.predict({'tweet': X_val_text, 'target': X_val_target})
y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report and confusion matrix
report = classification_report(val_data['Stance'], y_pred, target_names=class_labels, zero_division=1)  # Or zero_division=0.5

confusion = confusion_matrix(val_data['Stance'], y_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""##Hyper Param tuning - Baseline BERT

### Run1  : Hyper Param Tuning[ Optimization & LR ]
"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix

# Define your BERT model function with learning rate and optimizer options
def build_custom_bert_model_M1(learning_rate=1e-5, optimizer='adam'):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweet')
    target_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='target')

    # BERT preprocessing layer
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

    # Encoder for text input
    text_encoder_inputs = preprocessing_layer(text_input)
    text_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_text')
    text_outputs = text_encoder(text_encoder_inputs)

    # Encoder for target input
    target_encoder_inputs = preprocessing_layer(target_input)
    target_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_target')
    target_outputs = target_encoder(target_encoder_inputs)

    # Concatenate the outputs
    merged = tf.keras.layers.concatenate([text_outputs['pooled_output'], target_outputs['pooled_output']])

    # Additional layers
    net = tf.keras.layers.Dropout(0.1)(merged)
    net = tf.keras.layers.Dense(3, activation='softmax', name='stance')(net)

    # Create an optimizer based on the provided parameter
    if optimizer == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    return tf.keras.Model(inputs=[text_input, target_input], outputs=net)

# Create your custom BERT model with desired learning rate and optimizer
learning_rate = 1e-4  # Adjust the learning rate as needed
optimizer_choice = 'adam'  # Choose 'adam' or 'sgd' as the optimizer

bert_hyperparam_model = build_custom_bert_model_M1(learning_rate=learning_rate, optimizer=optimizer_choice)

# Compile the model
bert_hyperparam_model.compile(optimizer='adam',
                          loss='categorical_crossentropy',
                          metrics=['accuracy'])

# Split your data into training and validation
train_data, val_data = train_test_split(data_train, test_size=0.2, random_state=42)

# Encode the 'Stance' labels using one-hot encoding
label_encoder = LabelEncoder()
train_data['Stance'] = label_encoder.fit_transform(train_data['Stance'])
val_data['Stance'] = label_encoder.transform(val_data['Stance'])

# Limit the number of training samples (you can adjust this number)
# This can be useful for debugging or quicker training
# Only include this line if you want to limit the training data
train_data = train_data.sample(n=1000, random_state=42)

# Define your input and target data
X_train_text = train_data['Tweet'].values
X_train_target = train_data['Target'].values
y_train = to_categorical(train_data['Stance'].values, num_classes=3)

X_val_text = val_data['Tweet'].values
X_val_target = val_data['Target'].values
y_val = to_categorical(val_data['Stance'].values, num_classes=3)

# Train your model with a reduced batch size
history = bert_hyperparam_model.fit(
    {'tweet': X_train_text, 'target': X_train_target},
    {'stance': y_train},
    validation_data=({'tweet': X_val_text, 'target': X_val_target}, {'stance': y_val}),
    epochs=15,  # You can adjust the number of epochs
    batch_size=32  # Adjust the batch size to your available resources
)

# Extract training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot training and validation loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label="Training Loss")
plt.plot(validation_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label="Training Accuracy")
plt.plot(validation_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Validation Accuracy")

plt.tight_layout()
plt.show()

# Define class labels
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']

# Make predictions on the validation set
y_pred = bert_hyperparam_model.predict({'tweet': X_val_text, 'target': X_val_target})
y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report and confusion matrix
report = classification_report(val_data['Stance'], y_pred, target_names=class_labels, zero_division=1)  # Or zero_division=0.5
confusion = confusion_matrix(val_data['Stance'], y_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""### Run 2 : Hyper Param Tuning[ Regularization &  Early Stopping]
Hyperparameter Tuning: The learning rate, optimizer, batch size, and the number of epochs have been adjusted. The new values are:

Learning Rate: 1e-4
Optimizer: Adam
Batch Size: 64
Number of Epochs: 10
Regularization: L2 regularization with a strength of 1e-5 has been applied to the model.
I added the kernel_regularizer argument to the dense layer, which will apply L2 regularization with a strength of 1e-5 to the kernel (weights) of that layer.

Early Stopping: A callback for early stopping with a patience of 3 and restoring the best weights has been added during model training.

Comprehensive Evaluation: The code now provides a comprehensive evaluation report with classification metrics including precision, recall, F1-score, and a confusion matrix.
"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix

# Define your BERT model function with learning rate and optimizer options
def build_custom_bert_model_M2(learning_rate=1e-5, optimizer='adam'):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweet')
    target_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='target')

    # BERT preprocessing layer
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

    # Encoder for text input
    text_encoder_inputs = preprocessing_layer(text_input)
    text_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_text')
    text_outputs = text_encoder(text_encoder_inputs)

    # Encoder for target input
    target_encoder_inputs = preprocessing_layer(target_input)
    target_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_target')
    target_outputs = target_encoder(target_encoder_inputs)

    # Concatenate the outputs
    merged = tf.keras.layers.concatenate([text_outputs['pooled_output'], target_outputs['pooled_output']])

    # Apply L2 regularization to the dense layer
    dense_layer = tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(1e-5), name='stance')(merged)

    # Create an optimizer based on the provided parameter
    if optimizer == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    return tf.keras.Model(inputs=[text_input, target_input], outputs=dense_layer)

# Create your custom BERT model with desired learning rate and optimizer
learning_rate = 1e-4  # Adjust the learning rate as needed
optimizer_choice = 'adam'  # Choose 'adam' or 'sgd' as the optimizer

bert_hyperparam_model = build_custom_bert_model_M2(learning_rate=learning_rate, optimizer=optimizer_choice)



# Compile the model with optional L2 regularization
regularization = tf.keras.regularizers.l2(1e-5)
bert_hyperparam_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Split your data into training and validation
train_data, val_data = train_test_split(data_train, test_size=0.2, random_state=42)

# Encode the 'Stance' labels using one-hot encoding
label_encoder = LabelEncoder()
train_data['Stance'] = label_encoder.fit_transform(train_data['Stance'])
val_data['Stance'] = label_encoder.transform(val_data['Stance'])

# Limit the number of training samples (you can adjust this number)
# Only include this line if you want to limit the training data
train_data = train_data.sample(n=1000, random_state=42)

# Define your input and target data
X_train_text = train_data['Tweet'].values
X_train_target = train_data['Target'].values
y_train = to_categorical(train_data['Stance'].values, num_classes=3)

X_val_text = val_data['Tweet'].values
X_val_target = val_data['Target'].values
y_val = to_categorical(val_data['Stance'].values, num_classes=3)

# Train your model with updated hyperparameters
history = bert_hyperparam_model.fit(
    {'tweet': X_train_text, 'target': X_train_target},
    {'stance': y_train},
    validation_data=({'tweet': X_val_text, 'target': X_val_target}, {'stance': y_val}),
    epochs=20,
    batch_size=32,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)],
)

# Extract training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot training and validation loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label="Training Loss")
plt.plot(validation_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label="Training Accuracy")
plt.plot(validation_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Validation Accuracy")

plt.tight_layout()
plt.show()

# Define class labels
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']

# Make predictions on the validation set
y_pred = bert_hyperparam_model.predict({'tweet': X_val_text, 'target': X_val_target})
y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report and confusion matrix
report = classification_report(val_data['Stance'], y_pred, target_names=class_labels, zero_division=1)
confusion = confusion_matrix(val_data['Stance'], y_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""### Run 3 : Data Augmentation

"""

!pip install nlpaug

!pip install transformers

!pip install nlpaug
!python -m nlpaug.cli.download_model bert-base-uncased

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
import nlpaug.augmenter.word as naw
import nlpaug.flow as naf

# Define your BERT model function with learning rate and optimizer options
def build_custom_bert_model(learning_rate=1e-5, optimizer='adam'):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweet')
    target_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='target')

    # BERT preprocessing layer
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

    # Encoder for text input
    text_encoder_inputs = preprocessing_layer(text_input)
    text_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_text')
    text_outputs = text_encoder(text_encoder_inputs)

    # Encoder for target input
    target_encoder_inputs = preprocessing_layer(target_input)
    target_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_target')
    target_outputs = target_encoder(target_encoder_inputs)

    # Concatenate the outputs
    merged = tf.keras.layers.concatenate([text_outputs['pooled_output'], target_outputs['pooled_output']])

    # Apply L2 regularization to the dense layer
    dense_layer = tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(1e-5), name='stance')(merged)

    # Create an optimizer based on the provided parameter
    if optimizer == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    return tf.keras.Model(inputs=[text_input, target_input], outputs=dense_layer)

# Create your custom BERT model with desired learning rate and optimizer
learning_rate = 1e-4  # Adjust the learning rate as needed
optimizer_choice = 'adam'  # Choose 'adam' or 'sgd' as the optimizer

bert_hyperparam_model = build_custom_bert_model(learning_rate=learning_rate, optimizer=optimizer_choice)

# Compile the model with optional L2 regularization
regularization = tf.keras.regularizers.l2(1e-5)
bert_hyperparam_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Split your data into training and validation
train_data, val_data = train_test_split(data_train, test_size=0.2, random_state=42)

# Encode the 'Stance' labels using one-hot encoding
label_encoder = LabelEncoder()
train_data['Stance'] = label_encoder.fit_transform(train_data['Stance'])
val_data['Stance'] = label_encoder.transform(val_data['Stance'])

# Limit the number of training samples (you can adjust this number)
# Only include this line if you want to limit the training data
train_data = train_data.sample(n=1000, random_state=42)

# Define your input and target data
X_train_text = train_data['Tweet'].values
X_train_target = train_data['Target'].values
y_train = to_categorical(train_data['Stance'].values, num_classes=3)

X_val_text = val_data['Tweet'].values
X_val_target = val_data['Target'].values
y_val = to_categorical(val_data['Stance'].values, num_classes=3)

# Data augmentation using nlpaug
aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action="substitute")
augmented_X_train_text = aug.augment(X_train_text, n=1)  # Augment only 10% of data

# Combine original and augmented data
combined_X_train_text = X_train_text + augmented_X_train_text

# Train your model with updated hyperparameters
history = bert_hyperparam_model.fit(
    {'tweet': combined_X_train_text, 'target': X_train_target},
    {'stance': y_train},
    validation_data=({'tweet': X_val_text, 'target': X_val_target}, {'stance': y_val}),
    epochs=15,
    batch_size=32,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)],
)

# Extract training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot training and validation loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label="Training Loss")
plt.plot(validation_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label="Training Accuracy")
plt.plot(validation_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Validation Accuracy")

plt.tight_layout()
plt.show()

# Define class labels
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']

# Make predictions on the validation set
y_pred = bert_hyperparam_model.predict({'tweet': X_val_text, 'target': X_val_target})
y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report and confusion matrix
report = classification_report(val_data['Stance'], y_pred, target_names=class_labels, zero_division=1)
confusion = confusion_matrix(val_data['Stance'], y_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""### Run 4 : Class weight balancing"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix

# Define your BERT model function with learning rate and optimizer options
def build_custom_bert_model_M4(learning_rate=1e-5, optimizer='adam'):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweet')
    target_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='target')

    # BERT preprocessing layer
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

    # Encoder for text input
    text_encoder_inputs = preprocessing_layer(text_input)
    text_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_text')
    text_outputs = text_encoder(text_encoder_inputs)

    # Encoder for target input
    target_encoder_inputs = preprocessing_layer(target_input)
    target_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_target')
    target_outputs = target_encoder(target_encoder_inputs)

    # Concatenate the outputs
    merged = tf.keras.layers.concatenate([text_outputs['pooled_output'], target_outputs['pooled_output']])

    # Apply L2 regularization to the dense layer
    dense_layer = tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(1e-5), name='stance')(merged)

    # Create an optimizer based on the provided parameter
    if optimizer == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    return tf.keras.Model(inputs=[text_input, target_input], outputs=dense_layer)

# Load your data (data_train) and define tfhub_handle_preprocess and tfhub_handle_encoder as needed


# Split your data into training and validation
train_data, val_data = train_test_split(data_train, test_size=0.2, random_state=42)

# Encode the 'Stance' labels using one-hot encoding
label_encoder = LabelEncoder()
train_data['Stance'] = label_encoder.fit_transform(train_data['Stance'])
val_data['Stance'] = label_encoder.transform(val_data['Stance'])

# Limit the number of training samples (you can adjust this number)
train_data = train_data.sample(n=1000, random_state=42)

# Define your input and target data
X_train_text = train_data['Tweet'].values
X_train_target = train_data['Target'].values
y_train = to_categorical(train_data['Stance'].values, num_classes=3)

X_val_text = val_data['Tweet'].values
X_val_target = val_data['Target'].values
y_val = to_categorical(val_data['Stance'].values, num_classes=3)

# Create your custom BERT model with desired learning rate and optimizer
learning_rate = 1e-4  # Adjust the learning rate as needed
optimizer_choice = 'adam'  # Choose 'adam' or 'sgd' as the optimizer

bert_hyperparam_model = build_custom_bert_model_M4(learning_rate=learning_rate, optimizer=optimizer_choice)

# Compile the model without class weights
bert_hyperparam_model.compile(
    optimizer=optimizer_choice,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train your model with updated hyperparameters
history = bert_hyperparam_model.fit(
    {'tweet': X_train_text, 'target': X_train_target},
    {'stance': y_train},
    validation_data=({'tweet': X_val_text, 'target': X_val_target}, {'stance': y_val}),
    epochs=30,  # You can adjust the number of epochs
    batch_size=32  # Adjust the batch size to your available resources
)

# Extract training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot training and validation loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label="Training Loss")
plt.plot(validation_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label="Training Accuracy")
plt.plot(validation_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Validation Accuracy")

plt.tight_layout()
plt.show()

# Define class labels
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']

# Make predictions on the validation set
y_pred = bert_hyperparam_model.predict({'tweet': X_val_text, 'target': X_val_target})
y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report and confusion matrix
report = classification_report(val_data['Stance'], y_pred, target_names=class_labels, zero_division=1)
confusion = confusion_matrix(val_data['Stance'], y_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""### Run 5 :  Oversampling of data"""

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Define your BERT model function with learning rate and optimizer options
def build_custom_bert_model_M4(learning_rate=1e-5, optimizer='adam'):
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweet')
    target_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='target')

    # BERT preprocessing layer
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')

    # Encoder for text input
    text_encoder_inputs = preprocessing_layer(text_input)
    text_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_text')
    text_outputs = text_encoder(text_encoder_inputs)

    # Encoder for target input
    target_encoder_inputs = preprocessing_layer(target_input)
    target_encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder_target')
    target_outputs = target_encoder(target_encoder_inputs)

    # Concatenate the outputs
    merged = tf.keras.layers.concatenate([text_outputs['pooled_output'], target_outputs['pooled_output']])

    # Apply L2 regularization to the dense layer
    dense_layer = tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(1e-5), name='stance')(merged)

    # Create an optimizer based on the provided parameter
    if optimizer == 'adam':
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer == 'sgd':
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    return tf.keras.Model(inputs=[text_input, target_input], outputs=dense_layer)

# Load your data (data_train) and define tfhub_handle_preprocess and tfhub_handle_encoder as needed
# ...

# Split your data into training and validation
train_data, val_data = train_test_split(data_train, test_size=0.2, random_state=42)

# Encode the 'Stance' labels using one-hot encoding
label_encoder = LabelEncoder()
train_data['Stance'] = label_encoder.fit_transform(train_data['Stance'])
val_data['Stance'] = label_encoder.transform(val_data['Stance'])

# Define your input and target data
X_train_text = train_data['Tweet'].values
X_train_target = train_data['Target'].values
y_train = to_categorical(train_data['Stance'].values, num_classes=3)

X_val_text = val_data['Tweet'].values
X_val_target = val_data['Target'].values
y_val = to_categorical(val_data['Stance'].values, num_classes=3)

# Oversample stances other than "AGAINST" using SMOTE
smote = SMOTE(sampling_strategy={'FAVOR': 1000, 'NONE': 1000}, random_state=42)

# Resample the "FAVOR" and "NONE" classes in the training data
X_train_text_minority = X_train_text[train_data['Stance'] != 0]
X_train_target_minority = X_train_target[train_data['Stance'] != 0]
y_train_minority = y_train[train_data['Stance'] != 0]

X_train_text_resampled, X_train_target_resampled, y_train_resampled = smote.fit_resample(
    [X_train_text_minority, X_train_target_minority],
    y_train_minority
)

# Combine the resampled data with the "AGAINST" stance data
X_train_text_resampled = np.concatenate([X_train_text_resampled, X_train_text[train_data['Stance'] == 0]], axis=0)
X_train_target_resampled = np.concatenate([X_train_target_resampled, X_train_target[train_data['Stance'] == 0]], axis=0)
y_train_resampled = np.concatenate([y_train_resampled, y_train[train_data['Stance'] == 0]], axis=0)

# Create your custom BERT model with desired learning rate and optimizer
learning_rate = 1e-4  # Adjust the learning rate as needed
optimizer_choice = 'adam'  # Choose 'adam' or 'sgd' as the optimizer

bert_hyperparam_model = build_custom_bert_model_M4(learning_rate=learning_rate, optimizer=optimizer_choice)

# Compile the model without class weights
bert_hyperparam_model.compile(
    optimizer=optimizer_choice,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Train your model with updated hyperparameters
history = bert_hyperparam_model.fit(
    {'tweet': X_train_text_resampled, 'target': X_train_target_resampled},
    {'stance': y_train_resampled},
    validation_data=({'tweet': X_val_text, 'target': X_val_target}, {'stance': y_val}),
    epochs=30,  # You can adjust the number of epochs
    batch_size=32  # Adjust the batch size to your available resources
)

# Extract training history
training_loss = history.history['loss']
validation_loss = history.history['val_loss']
training_accuracy = history.history['accuracy']
validation_accuracy = history.history['val_accuracy']

# Plot training and validation loss curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label="Training Loss")
plt.plot(validation_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Training and Validation Loss")

# Plot training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label="Training Accuracy")
plt.plot(validation_accuracy, label="Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training and Validation Accuracy")

plt.tight_layout()
plt.show()

# Define class labels
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']

# Make predictions on the validation set
y_pred = bert_hyperparam_model.predict({'tweet': X_val_text, 'target': X_val_target})
y_pred = np.argmax(y_pred, axis=1)

# Compute the classification report and confusion matrix
report = classification_report(val_data['Stance'], y_pred, target_names=class_labels, zero_division=1)
confusion = confusion_matrix(val_data['Stance'], y_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""## Test the Model"""

# Apply the same preprocessing to the 'Tweet' column in the test data
data_test['Tweet'] = data_test['Tweet'].apply(preprocess_tweet)

# Define your input and target data for the test set
X_test_text = data_test['Tweet'].values
X_test_target = data_test['Target'].values

# Use the same label encoder that was fitted on the training data
y_test = to_categorical(label_encoder.transform(data_test['Stance'].values), num_classes=3)

# Evaluate your model on the test set
test_loss, test_accuracy = bert_hyperparam_model.evaluate(
    {'tweet': X_test_text, 'target': X_test_target},
    {'stance': y_test}
)

print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')

"""###Test Model Metrices"""

# Evaluate your model on the test set
test_loss, test_accuracy = custom_bert_model.evaluate(
    {'tweet': X_test_text, 'target': X_test_target},
    {'stance': y_test}
)

print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')

# Predict the labels on the test set
y_test_pred = bert_hyperparam_model.predict({'tweet': X_test_text, 'target': X_test_target})
y_test_pred = np.argmax(y_test_pred, axis=1)

# Import necessary libraries for classification report and confusion matrix
from sklearn.metrics import classification_report, confusion_matrix

# Define class labels
class_labels = ['AGAINST', 'FAVOR', 'NEITHER']

# Compute the classification report and confusion matrix
report = classification_report(np.argmax(y_test, axis=1), y_test_pred, target_names=class_labels)
confusion = confusion_matrix(np.argmax(y_test, axis=1), y_test_pred)

print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

"""##Conclusion  - BERT Results Analysis
Baseline Results (Epochs: 10)
Accuracy: 26%
Classification Report:
Precision, Recall, and F1-Score are particularly low for all classes.
Confusion Matrix: The model is not performing well, with many misclassifications.
Run 1: Hyperparameter Tuning - Baseline BERT (Epochs: 15)
Accuracy: 48%
Classification Report:
Improved accuracy, but still low precision, recall, and F1-Score for classes other than "AGAINST."
Run 2: Hyperparameter Tuning (Regularization & Early Stopping) (Epochs: 20)
Accuracy: 48%
Classification Report:
Similar to Run 1, with low precision, recall, and F1-Score for classes other than "AGAINST."
Run 3: Data Augmentation (No additional information provided)
Accuracy not mentioned.
Run 4: Class Weight Balancing (Epochs: 30)
Accuracy: 48%
Classification Report:
Similar to Runs 1 and 2, with low precision, recall, and F1-Score for classes other than "AGAINST."
Test Data Evaluation
The model is evaluated on the test data with an accuracy of 57%. However, the classification report shows low precision, recall, and F1-Score for classes other than "AGAINST."
Summary
The baseline BERT model performed poorly, with low accuracy and F1-Score.
Hyperparameter tuning improved accuracy but didn't address the low F1-Score for some classes.
Data augmentation and class weight balancing had similar results to the tuned models.
The models tend to predict "AGAINST" well but struggle with other classes.
There is room for further improvement, possibly by refining the model architecture, incorporating more data, or exploring advanced techniques.
"""

import matplotlib.pyplot as plt

# Extract the training history for the test data
test_training_loss = history.history['loss']
test_training_accuracy = history.history['accuracy']

# Plot training loss and accuracy curves for the test data
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(test_training_loss, label="Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("Test Training Loss")

plt.subplot(1, 2, 2)
plt.plot(test_training_accuracy, label="Training Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Test Training Accuracy")

plt.tight_layout()
plt.show()

"""## Model 2 - LSTM"""

# Commented out IPython magic to ensure Python compatibility.
#Import necessary packages
import pandas as pd
import numpy as np
import re
import string
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import matplotlib.pyplot as plt
import matplotlib.ticker as plticker
# %matplotlib inline
import seaborn as sns
import spacy
import os
import tensorflow as tf
AUTOTUNE = tf.data.experimental.AUTOTUNE
import tensorflow_datasets as tfds
import pathlib
import shutil
import tempfile
from tensorflow.keras.layers import Dense, Embedding, GRU, LSTM, Bidirectional,Dropout,Flatten
from tensorflow.keras.models import Sequential
from  IPython import display
from keras import backend as K

import pandas as pd
import re

# Preprocessing function
def preprocess_tweet(tweet):
    tweet = tweet.lower()
    tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet)
    tweet = re.sub(f"[{re.escape(string.punctuation)}]", '', tweet)
    tweet = re.sub(r'\d', '', tweet)
    return tweet

# Load train and test data
data_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/train.csv', sep=',', lineterminator='\r', encoding='unicode_escape')
data_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/test.csv', sep=',', lineterminator='\r', encoding='unicode_escape')

# Apply preprocessing to the 'Tweet' column in both train and test data
data_train['Tweet'] = data_train['Tweet'].apply(preprocess_tweet)
data_test['Tweet'] = data_test['Tweet'].apply(preprocess_tweet)

# Save the cleaned data as new CSV files
data_train.to_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/TrainFileCleaned.csv', index=False)
data_test.to_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/TestFileCleaned.csv', index=False)
#Filter out non target and retain only specified targets in test data
#TARGETS : {"Hillary Clinton", "Legalization of Abortion", "Atheism", "Climate Change is a Real Concern", "Feminist Movement"  }
data_test= pd.read_csv('/content/drive/My Drive/Colab Notebooks/StanceDataset/TestFileCleaned.csv',sep=',',encoding = 'unicode_escape')
filt=data_train['Target'].unique()
data_test = data_test[data_test.Target.isin(filt)]

data_train.head()

#Encode the target and stance to intgers for classification
encode_target = {
    "Hillary Clinton" : 0,
    "Legalization of Abortion"  : 1,
   "Atheism" : 2,
   "Climate Change is a Real Concern" : 3,
   "Feminist Movement"  : 4
}
encode_stance = {
    "AGAINST" : 0,
    "FAVOR"  : 2,
    "NONE" : 1,
}

#Apply encoding for Stance feature
data_train["Stance"] = data_train["Stance"].apply(lambda x: encode_stance[x])
data_test["Stance"] = data_test["Stance"].apply(lambda x: encode_stance[x])
data_train[:2]

#Apply encoding for Target feature
data_train["Target"] = data_train["Target"].apply(lambda x: encode_target[x])
data_train[:2]

#Replace '@' and '#' with a space in each tweet of train and test set and save to new lists
list_clean = list()
test_clean = list()
lines = data_train["Tweet"].values.tolist()
testlines = data_test["Tweet"].values.tolist()


for line in lines:

    line = re.sub(r'@','', line)
    line = re.sub(r'#','', line)
    words = line.split(" ")
    list_clean.append(words)

for line in testlines:

    line = re.sub(r'@','', line)
    line = re.sub(r'#','', line)
    words = line.split(" ")
    test_clean.append(words)

!pip install tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Tokenize each word in the cleaned train and test data

validation_split = 0.20
max_length = 20

tokenizer_obj = Tokenizer()
tokenizer_obj.fit_on_texts(list_clean)
sequences = tokenizer_obj.texts_to_sequences(list_clean)


tokenizer_obj1 = Tokenizer()
tokenizer_obj1.fit_on_texts(list_clean)
sequences1 = tokenizer_obj1.texts_to_sequences(test_clean)

word_index = tokenizer_obj.word_index
print("unique tokens - "+str(len(word_index)))
vocab_size = len(tokenizer_obj.word_index) + 1
print('vocab_size - '+str(vocab_size))

lines_pad = pad_sequences(sequences, maxlen=max_length,padding='post')
test_pad = pad_sequences(sequences1, maxlen=max_length,padding='post')

test_stance = data_test["Stance"].values
category_stance =  data_train["Stance"].values
category_target =  data_train["Target"].values

test_stance

#one hot encoding for target classes
n_values_stance = np.max(category_stance) + 1
n_values_targe = np.max(category_target) + 1
n_values_test = np.max(test_stance) + 1

Y_stance = np.eye(n_values_stance)[category_stance]
Y_target = np.eye(n_values_targe)[category_target]
Y_test = np.eye(n_values_test)[test_stance]

num_validation_samples = int(validation_split * lines_pad.shape[0])

#train dataset
X_train_pad = lines_pad[:-num_validation_samples]
y_train_target = Y_target[:-num_validation_samples]
y_train_stance = Y_stance[:-num_validation_samples]

#validation dataset
X_val_pad = lines_pad[-num_validation_samples:]
y_val_target = Y_target[-num_validation_samples:]
y_val_stance = Y_stance[-num_validation_samples:]

#Unseen test dataset
X_test_pad = test_pad
y_teststance = Y_test

#Print shape of train test split for both target and stance
print('Shape of X_train_pad:', X_train_pad.shape)
print('Shape of y_train_target:', y_train_target.shape)
print('Shape of y_train_stance:', y_train_stance.shape)

print('Shape of X_test_pad:', X_val_pad.shape)
print('Shape of y_test_target:', y_val_target.shape)
print('Shape of y_test_stance:', y_val_stance.shape)

# Commented out IPython magic to ensure Python compatibility.
#Load the TensorBoard
logdir = pathlib.Path(tempfile.mkdtemp())/"JKtensorboard_logs"
shutil.rmtree(logdir, ignore_errors=True)

# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Open an embedded TensorBoard viewer
# %tensorboard --logdir {logdir}/models

#Load the pretrained glove twitter weight matrix
file = open('/content/drive/My Drive/Colab Notebooks/glove.twitter.27B.25d.txt', encoding='utf-8')

glove_vectors = dict()
for line in file:
  values = line.split()
  word = values[0]
  features = np.asarray(values[1:])
  glove_vectors[word] = features

# Print some of the loaded GloVe word vectors
num_vectors_to_print = 5

for i, (word, embedding) in enumerate(glove_vectors.items()):
    print(f'Word: {word}, Embedding: {embedding}')
    if i >= num_vectors_to_print - 1:
        break

file.close()

import numpy as np

# Load the pretrained GloVe Twitter weight matrix
file_path = '/content/drive/My Drive/Colab Notebooks/glove.twitter.27B.25d.txt'

glove_vectors = dict()

with open(file_path, encoding='utf-8') as file:
    for line in file:
        values = line.split()
        word = values[0]
        features = np.array(values[1:], dtype=float)  # Convert features to float
        glove_vectors[word] = features


# Define your word_index dictionary based on your tokenization

# Initialize the embedding matrix E_T with zeros
embedding_dim = 25  # Assuming you are using 25-dimensional GloVe vectors
E_T = np.zeros((len(word_index) + 1, embedding_dim))

# Fill the embedding matrix with GloVe vectors for words in your vocabulary
for word, i in word_index.items():
    embedding_vector = glove_vectors.get(word)
    if embedding_vector is not None and embedding_vector.size > 0:
        E_T[i] = embedding_vector

# E_T now contains the word embeddings from GloVe for words in your vocabulary

from tensorflow.keras.optimizers import Adam

#Define decay learning rate stratergy for Adam optimizer
def learn_adam(lr):
  STEPS_PER_EPOCH = 73
  lr = lr

  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    lr,
    decay_steps=STEPS_PER_EPOCH*1000,
    decay_rate=10,
    staircase=False)
  #Visualise the learning rate curve for Adam optimizer
  step = np.linspace(0,100000)
  lr = lr_schedule(step)
  plt.figure(figsize = (8,6))
  plt.plot(step/STEPS_PER_EPOCH, lr)
  plt.ylim([0,max(plt.ylim())])
  plt.xlabel('Epoch')
  plt.ylabel('Learning Rate')

  optimizer = Adam(learning_rate=lr_schedule)
  return optimizer

optimizer = learn_adam(0.001)

#Specify training epochs
EPOCH = 2

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Embedding

def get_model_target(reg_lambda, drop):
    E_T = np.zeros((len(word_index) + 1, 25))
    for word, i in word_index.items():
        embedding_vector = glove_vectors.get(word)
        if embedding_vector is not None and len(embedding_vector) == 25:
            E_T[i] = embedding_vector

    embedding_layer_TL = Embedding(len(word_index) + 1,
                                   25,
                                   weights=[E_T],
                                   input_length=max_length,
                                   trainable=False)

    model_glove = Sequential()
    model_glove.add(embedding_layer_TL)
    model_glove.add(Bidirectional(LSTM(units=128, dropout=drop, recurrent_dropout=drop, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))))
    model_glove.add(Flatten())
    model_glove.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)))
    model_glove.add(Dropout(drop))
    model_glove.add(Dense(5, activation='softmax'))

    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

    model_glove.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy', f1_m])

    return model_glove

#Selection of dropouts and lambda values for model training
m_histories = {}

#Hyper parameters tuning tested after random search
# dropout_rate = [0.5,0.7, 0.8,0.9]
# lambda_vals = [ 0.05,0.001,0.005, 0.001, 0.0005]

#Fine tuning the hyper parameters on trainable layers
dropout_rate = [0.7, 0.8]
lambda_vals = [ 0.005, 0.001, 0.0005]

#Specify location to save model checkpoints
checkpoint_path = '/content/drive/My Drive/Colab Notebooks/'
checkpoint_dir = os.path.dirname(checkpoint_path)

#Function defintion to calculate the metrics
def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))


def get_callbacks(name,early_stop=True):
  if early_stop:
      return [
              tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,restore_best_weights=True),
              tf.keras.callbacks.TensorBoard(logdir/name, histogram_freq=60, embeddings_freq=60),
              tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path+name,
                                                 save_weights_only=True,
                                                 verbose=1)

      ]

  else:
    return [tf.keras.callbacks.TensorBoard(logdir/name)]

import tensorflow as tf
from sklearn.metrics import f1_score


for reg_lambda in lambda_vals:
  for drop in dropout_rate:
   tf.keras.backend.clear_session()
   model_target = get_model_target(reg_lambda, drop)
   m_histories['with_TL'+ '_h_' + str(reg_lambda)+str(drop)] = model_target.fit(X_train_pad, y_train_target, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val_target), callbacks=get_callbacks('models/with_TL'+ '_h_' + str(reg_lambda),early_stop=True), verbose=1)

import pandas as pd

# Define a function to store the lambda, dropout, validation, and train losses for the best scores and the latest epoch
def train_valid_scores(name, lambda_vals, dropout_rate, no_run):
    metric = 'loss'
    met = 'f1_m'
    l_train = list()
    l_val = list()
    reg = list()
    dropo = list()
    val_acc = list()

    for reg_lambda in lambda_vals:
        for drop in dropout_rate:
            l_train.append(m_histories[name + str(reg_lambda) + str(drop)].history[metric][-1])  # Latest epoch
            l_val.append(m_histories[name + str(reg_lambda) + str(drop)].history['val_' + metric][-1])  # Latest epoch
            val_acc.append(m_histories[name + str(reg_lambda) + str(drop)].history['val_' + met][-1])  # Latest epoch
            reg.append(reg_lambda)
            dropo.append(drop)

    d = {'l_train': l_train, 'l_val': l_val, 'reg': reg, 'dropo': dropo, 'val_acc': val_acc}

    df = pd.DataFrame(data=d)
    return df

# Store the latest scores
df1 = train_valid_scores('with_TL' + '_h_', lambda_vals, dropout_rate, -1)
print("Latest Scores:")
print(df1)

# Store the best scores (you might need to define "best" based on your evaluation metric)
df2 = train_valid_scores('with_TL' + '_h_', lambda_vals, dropout_rate, -10)
print("\nBest Scores:")
print(df2)

#Print the latest scores
df1 = train_valid_scores('with_TL'+ '_h_',lambda_vals,dropout_rate,-1)
df1

from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

#Visulaize the Loss,Lambda and dropout for each combinations

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(df1['reg'], df1['dropo'], df1['l_train'], c='r', marker='o')
ax.scatter(df1['reg'], df1['dropo'], df1['l_val'], c='b', marker='o')

ax.set_xlabel('lambda')
ax.set_ylabel('drop_out')
ax.set_zlabel('Loss')

plt.show()

from keras.layers import RepeatVector,Flatten

#Model training with best hyperparameters for TARGET

model_glove_final= get_model_target(0.0005,0.8)
m_histories['with_TL'+ '_h_'+'final'] = model_glove_final.fit(X_train_pad, y_train_target, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val_target), callbacks=get_callbacks('models/with_TL'+ '_h_' + 'final',early_stop=True), verbose=1)

epoch = 10

#Unfreeze the first layer for fine tuning
for layer in model_glove_final.layers[:1]:
    layer.trainable = True


optimizer1 = learn_adam(0.0005)

#Fine tune TARGET model for 5 epochs
model_glove_final.compile(loss='CategoricalCrossentropy', optimizer=optimizer1, metrics=['categorical_accuracy',f1_m])
m_histories['with_TL'+ '_h_'+'finalFineTuned'] = model_glove_final.fit(X_train_pad, y_train_target, batch_size=32, epochs=5, validation_data=(X_val_pad, y_val_target), callbacks=get_callbacks('models/with_TL'+ '_h_' + 'finalFineTuned',early_stop=True), verbose=1)

#Save model weights
model_glove_final.save_weights('/content/my_model_model_glove_final')

checkpoint_path = '/content/my_model_model_glove_final'

#Model definition for TARGET without pretrained weights , later added from checkpoint
model_glove_tar = Sequential()
embedding_layer_TL = Embedding(len(word_index) + 1,
                            25,
                            input_length=max_length,
                            trainable=False)
model_glove_tar.add(embedding_layer_TL)
model_glove_tar.add(Bidirectional(LSTM(units=128,  dropout=0.8, recurrent_dropout=0.8,return_sequences=True)))
model_glove_tar.add(Flatten())
model_glove_tar.add(Dense(128, activation='relu'))
model_glove_tar.add(Dropout(0.8))
model_glove_tar.add(Dense(5, activation='softmax'))

model_glove_tar.load_weights(checkpoint_path)

#Freezing all the layers for TARGET
for layer in model_glove_tar.layers[:]:
    layer.trainable = False

"""### Results of Target Classification Model

The model was fine-tuned with different hyperparameters, including learning rate, dropout rate, and L2 regularization.
The best hyperparameters for this model were selected based on validation accuracy.
After fine-tuning, the model achieved the following results (for the best hyperparameters):
Training Loss (F1_m): 1.6726
Training Categorical Accuracy: 0.3405
Validation Loss (F1_m): 1.6032
Validation Categorical Accuracy: 0.4811
"""

#Hyperparameter selection for STANCE
dropout_rate = [0.5,0.6,0.7,0.8]
lambda_vals = [ 0.005, 0.001, 0.0005]

#Training model for STANCE with combination of the hyper parameters

for reg_lambda in lambda_vals:
  for drop in dropout_rate:
   tf.keras.backend.clear_session()
   model=Sequential()
   #Remove the top 3 layers of pretrained target model and add other layers
   for layer in model_glove_tar.layers[:-3]:
     model.add(layer)
   model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)))
   model.add(Dropout(drop))
   model.add(Dense(3, activation='softmax'))
   for layer in model.layers[:3]:
    layer.trainable = False
   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy',f1_m])
   m_histories['with_TL'+ '_stance_' + str(reg_lambda)+str(drop)] = model.fit(X_train_pad, y_train_stance, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val_stance), callbacks=get_callbacks('models/with_TL'+ '_stance_' + str(reg_lambda)+str(drop),early_stop=True), verbose=1)

#Print the latest scores
df3 = train_valid_scores('with_TL'+ '_stance_',lambda_vals,dropout_rate,-1)
df3

#Print the best scores
df4 = train_valid_scores('with_TL'+ '_stance_',lambda_vals,dropout_rate,-10)
df4

#Train model with the best hyperparameter obtained
tf.keras.backend.clear_session()
model=Sequential()
 #Remove the top 3 layers of pretrained target model and add other layers
for layer in model_glove_tar.layers[:-3]:
     model.add(layer)
model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005)))
model.add(Dropout(0.5))
model.add(Dense(3, activation='softmax'))

#Fit the model
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy',f1_m])
m_histories['with_TL'+ '_stance_' ] = model.fit(X_train_pad, y_train_stance, batch_size=32, epochs=EPOCH, validation_data=(X_val_pad, y_val_stance), callbacks=get_callbacks('models/with_TL'+ '_stance_',early_stop=True), verbose=1)

# Define the optimizer
optimizer1 = tf.keras.optimizers.Adam(learning_rate=0.001)

# Build the optimizer with the model's trainable variables
optimizer1.build(model.trainable_variables)

# Compile the model with the appropriate loss and the built optimizer
model.compile(loss='CategoricalCrossentropy', optimizer=optimizer1, metrics=['categorical_accuracy', f1_m])

# Fine-tune the model for 5 epochs
fine_tune_history = model.fit(X_train_pad, y_train_stance, batch_size=32, epochs=5, validation_data=(X_val_pad, y_val_stance), callbacks=get_callbacks('models/with_TL' + '_h_' + 'finalFineTunedStance', early_stop=True), verbose=1)

#Unfreeeze all the layers of the model for fine tuning
for layer in model.layers[:]:
    layer.trainable = True

#Save STANCE model weights
model.save_weights('./checkpoints/model_stance_final')

"""###Results of Stance Classification Model:

Similar hyperparameter tuning was performed for the stance classification model.
The model was trained and fine-tuned with different hyperparameters.
The best hyperparameters for this model were selected based on validation accuracy.
After fine-tuning, the model achieved the following results (for the best hyperparameters):
Training Loss (F1_m): 1.1132
Training Categorical Accuracy: 0.4987
Validation Loss (F1_m): 1.0711
Validation Categorical Accuracy: 0.5292
"""

#Predict the STANCE for unseen data
ynew = model.predict(X_test_pad)
pred = np.zeros_like(ynew)
pred[np.arange(len(ynew)), ynew.argmax(1)] = 1
pred

#Convert the actual stance to float32
y_teststance = np.float32(y_teststance)
print(f1_m(pred,y_teststance))

#Print macro F1 score
print(f1_m(pred,y_teststance))

#converting one hot encoding to integer classes 0,1,2
pred1_int = np.argmax(pred, axis=1)
y_teststance1 = np.argmax(y_teststance, axis=1)

import sklearn
#Print the confusion matrix for 3 classes
sklearn.metrics.confusion_matrix(pred1_int,y_teststance1)

#print the summary of matrix for each classes
sklearn.metrics.classification_report(pred1_int,y_teststance1,output_dict=True)

"""##Model Test : Unseen Data Evaluation

The models were evaluated on unseen data.
The F1-Score, particularly the macro F1-Score, was reported as a performance metric.
The macro F1-Score for the stance model was reported to be approximately 0.6069.

Summary:

The target classification model achieved an F1-Score of around 1.6726 (training) and 1.6032 (validation) for the best hyperparameters.
The stance classification model achieved an F1-Score of approximately 1.1132 (training) and 1.0711 (validation) for the best hyperparameters.
The macro F1-Score for the stance model on unseen data was approximately 0.6069.

These scores indicate the model's performance in terms of classification accuracy and the ability to generalize to unseen data.

# Overall Conclusion



The LSTM model's performance was relatively better compared to the target classification task, where the F1-Score was much lower



prefer to have 128 units for both LSTM and BidirectionalLSTM layers. Along with this, a dropout and recurrent dropout ratio of 0.2 and L2 regularization of 0.0001 was the optimal hyper-parameter setting for our approach. The learning rate 0.001 of Adam optimizer was the best for the task. We aimed to reduce the generalization gap by improving the accuracy and reducing the loss between the train and validation curves. However, the vocabulary for training and testing data and evaluation data had some variations. Few words which are present in the test data and evaluation data are not being used in the training set, creating a bias towards the prediction.



The tweets data had a very limited number of textual contents for this task to be completed adequately. Therefore, it is difficult to derive some conclusions for this model if the data density is increased. On the contrary, improved data would be helpful to design a newer architecture using our proposed approach and tend to achieve some higher accuracies across classes



In summary, the LSTM model outperformed the BERT model for tweet stance classification. Specific hyperparameters were fine-tuned, and efforts to reduce the generalization gap were made. However, data limitations with limited textual content were observed. Improved data density is essential for designing more robust models and achieving higher accuracies


In summary, our findings indicate that the LSTM model demonstrated superior performance in the context of tweet stance classification compared to the BERT model. The LSTM model was optimized with specific hyperparameters: it benefitted from having 128 units for both the LSTM and Bidirectional LSTM layers, a dropout and recurrent dropout ratio of 0.2, and L2 regularization at 0.0001. Additionally, the model's performance was maximized with an Adam optimizer learning rate of 0.001.

My primary objective was to narrow the generalization gap by enhancing model accuracy and reducing the loss between training and validation data. However, we encountered challenges stemming from vocabulary variations between training, testing, and evaluation datasets. This discrepancy in word usage introduced a bias in our predictions, necessitating careful consideration in handling such discrepancies.

Moreover, it is worth noting that the tweet data available for this task contained a relatively limited amount of textual content, which could have impacted the model's ability to achieve optimal performance. Thus, drawing robust conclusions from our model's performance may be somewhat constrained by data limitations.

In conclusion, the LSTM model emerged as the more effective choice for tweet stance classification. Still, the success of our approach could significantly benefit from improved data density, offering the potential to design more advanced architectures and achieve higher accuracies across various stance classes.
"""